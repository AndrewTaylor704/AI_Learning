{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "Length:  533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length:  616\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# Tokenisation - GPT\n",
    "\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "print('---')\n",
    "print(text)\n",
    "print('Length: ', len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print('Length: ', len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v,k) for k, v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length: 596\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints from the input text, replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we're not at the very last position and the pair matches, replace it\n",
    "        if i < len(ids) -1 and ids[i] == pair[0] and ids [i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i+=1\n",
    "    return newids\n",
    "\n",
    "#print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print('Length:', len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = '''A Programmer’s Introduction to Unicode\n",
    "March 3, 2017 · Coding · 25 Comments\n",
    "\n",
    "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
    "\n",
    "A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.\n",
    "\n",
    "I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\n",
    "\n",
    "Diversity and Inherent Complexity\n",
    "The Unicode Codespace\n",
    "Codespace Allocation\n",
    "Scripts\n",
    "Usage Frequency\n",
    "Encodings\n",
    "UTF-8\n",
    "UTF-16\n",
    "Combining Marks\n",
    "Canonical Equivalence\n",
    "Normalization Forms\n",
    "Grapheme Clusters\n",
    "And More…\n",
    "Diversity and Inherent Complexity\n",
    "As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.\n",
    "\n",
    "When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”\n",
    "\n",
    "However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.\n",
    "\n",
    "Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.\n",
    "\n",
    "Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.\n",
    "\n",
    "Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!\n",
    "\n",
    "The Unicode Codespace\n",
    "Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.\n",
    "\n",
    "The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.\n",
    "\n",
    "Codespace Allocation\n",
    "To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.\n",
    "\n",
    "Map of the Unicode codespace (click to zoom)\n",
    "\n",
    "White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.\n",
    "\n",
    "Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.\n",
    "\n",
    "(In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)\n",
    "\n",
    "Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.\n",
    "\n",
    "Scripts\n",
    "Let’s zoom in on the first three planes, since that’s where the action is:\n",
    "\n",
    "Map of scripts in Unicode planes 0–2 (click to zoom)\n",
    "\n",
    "This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.\n",
    "\n",
    "Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).\n",
    "\n",
    "Usage Frequency\n",
    "One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.\n",
    "\n",
    "Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)\n",
    "\n",
    "You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.\n",
    "\n",
    "Encodings\n",
    "We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?\n",
    "\n",
    "The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.\n",
    "\n",
    "Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.\n",
    "\n",
    "Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.\n",
    "\n",
    "UTF-8\n",
    "In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.\n",
    "\n",
    "UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:\n",
    "\n",
    "UTF-8 (binary)\tCode point (binary)\tRange\n",
    "0xxxxxxx\txxxxxxx\tU+0000–U+007F\n",
    "110xxxxx 10yyyyyy\txxxxxyyyyyy\tU+0080–U+07FF\n",
    "1110xxxx 10yyyyyy 10zzzzzz\txxxxyyyyyyzzzzzz\tU+0800–U+FFFF\n",
    "11110xxx 10yyyyyy 10zzzzzz 10wwwwww\txxxyyyyyyzzzzzzwwwwww\tU+10000–U+10FFFF\n",
    "A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.\n",
    "\n",
    "Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.\n",
    "\n",
    "However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.\n",
    "\n",
    "UTF-16\n",
    "The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.\n",
    "\n",
    "Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:\n",
    "\n",
    "UTF-16 (binary)\tCode point (binary)\tRange\n",
    "xxxxxxxxxxxxxxxx\txxxxxxxxxxxxxxxx\tU+0000–U+FFFF\n",
    "110110xxxxxxxxxx 110111yyyyyyyyyy\txxxxxxxxxxyyyyyyyyyy + 0x10000\tU+10000–U+10FFFF\n",
    "A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.\n",
    "\n",
    "Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.\n",
    "\n",
    "Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)\n",
    "\n",
    "By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)\n",
    "\n",
    "Combining Marks\n",
    "In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!\n",
    "\n",
    "Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.\n",
    "\n",
    "In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.\n",
    "\n",
    "If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.\n",
    "\n",
    "For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.\n",
    "\n",
    "Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.\n",
    "\n",
    "Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓\n",
    "\n",
    "A few other places where dynamic character composition shows up in Unicode:\n",
    "\n",
    "Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.\n",
    "\n",
    "A Hebrew example, with niqqud:\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד\n",
    "Normal writing (no niqqud):\tאת דלתי הזיז הניע, קטב לשכתי ישוד\n",
    "Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “​ि” = “हि” (“h” + “i” = “hi”).\n",
    "\n",
    "Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”).\n",
    "\n",
    "Canonical Equivalence\n",
    "In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.\n",
    "\n",
    "Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.\n",
    "\n",
    "For example, the Vietnamese letter “ệ” can be expressed in five different ways:\n",
    "\n",
    "Fully precomposed: U+1EC7 “ệ”\n",
    "Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂”\n",
    "Partially precomposed: U+00EA “ê” + U+0323 “◌̣”\n",
    "Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂”\n",
    "Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣”\n",
    "Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!\n",
    "\n",
    "Normalization Forms\n",
    "To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).\n",
    "\n",
    "The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)\n",
    "\n",
    "The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).\n",
    "\n",
    "There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.\n",
    "\n",
    "Grapheme Clusters\n",
    "As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.\n",
    "\n",
    "UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.\n",
    "\n",
    "The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.\n",
    "\n",
    "Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.\n",
    "\n",
    "And More…\n",
    "There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.\n",
    "\n",
    "Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.\n",
    "\n",
    "Further reading:\n",
    "\n",
    "The Unicode Standard\n",
    "UTF-8 Everywhere Manifesto\n",
    "Dark corners of Unicode by Eevee\n",
    "ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things\n",
    "Python 3 Unicode Howto\n",
    "Google Noto Fonts—set of fonts intended to cover all assigned code points'''\n",
    "tokens = text.encode('utf-8')\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (105, 110) into a new token 257\n",
      "merging (115, 32) into a new token 258\n",
      "merging (116, 104) into a new token 259\n",
      "merging (101, 114) into a new token 260\n",
      "merging (99, 111) into a new token 261\n",
      "merging (116, 32) into a new token 262\n",
      "merging (226, 128) into a new token 263\n",
      "merging (44, 32) into a new token 264\n",
      "merging (97, 110) into a new token 265\n",
      "merging (111, 114) into a new token 266\n",
      "merging (100, 32) into a new token 267\n",
      "merging (97, 114) into a new token 268\n",
      "merging (101, 110) into a new token 269\n",
      "merging (257, 103) into a new token 270\n",
      "merging (261, 100) into a new token 271\n",
      "merging (121, 32) into a new token 272\n",
      "merging (97, 108) into a new token 273\n",
      "merging (259, 256) into a new token 274\n",
      "merging (111, 110) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276   # desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {} # mapping pairs onto new tokens\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256+i\n",
    "    print(f'merging {pair} into a new token {idx}')\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length:  24613\n",
      "ids length:  19489\n",
      "compression ratio: 1.26X\n"
     ]
    }
   ],
   "source": [
    "print('tokens length: ', len(tokens))\n",
    "print('ids length: ', len(ids))\n",
    "print(f'compression ratio: {len(tokens)/ len(ids):.2f}X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids, return Python string\n",
    "    tokens = b''.join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (tokens)\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "    \n",
    "print(encode('Hello world!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hello world')))\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "# GPT2 regex pattern to separate text strings into common apostrophe endings, words, \n",
    "# strings of numbers, punctuation and special characters, sequences of white spaces\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, 'Hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "A\n",
      "merge 1/256: (101, 32) -> 256 (b'e ') had 27643 occurrences\n",
      "merge 2/256: (116, 104) -> 257 (b'th') had 22739 occurrences\n",
      "merge 3/256: (116, 32) -> 258 (b't ') had 16508 occurrences\n",
      "merge 4/256: (115, 32) -> 259 (b's ') had 15364 occurrences\n",
      "merge 5/256: (100, 32) -> 260 (b'd ') had 14165 occurrences\n",
      "merge 6/256: (44, 32) -> 261 (b', ') had 14098 occurrences\n",
      "merge 7/256: (111, 117) -> 262 (b'ou') had 12730 occurrences\n",
      "merge 8/256: (101, 114) -> 263 (b'er') had 11771 occurrences\n",
      "merge 9/256: (105, 110) -> 264 (b'in') had 10606 occurrences\n",
      "merge 10/256: (121, 32) -> 265 (b'y ') had 10283 occurrences\n",
      "merge 11/256: (97, 110) -> 266 (b'an') had 10197 occurrences\n",
      "merge 12/256: (58, 10) -> 267 (b':\\n') had 8762 occurrences\n",
      "merge 13/256: (111, 114) -> 268 (b'or') had 8458 occurrences\n",
      "merge 14/256: (111, 32) -> 269 (b'o ') had 8134 occurrences\n",
      "merge 15/256: (101, 110) -> 270 (b'en') had 7568 occurrences\n",
      "merge 16/256: (10, 10) -> 271 (b'\\n\\n') had 7098 occurrences\n",
      "merge 17/256: (97, 114) -> 272 (b'ar') had 7081 occurrences\n",
      "merge 18/256: (32, 257) -> 273 (b' th') had 6662 occurrences\n",
      "merge 19/256: (111, 110) -> 274 (b'on') had 6435 occurrences\n",
      "merge 20/256: (108, 108) -> 275 (b'll') had 6357 occurrences\n",
      "merge 21/256: (104, 97) -> 276 (b'ha') had 6055 occurrences\n",
      "merge 22/256: (44, 10) -> 277 (b',\\n') had 5501 occurrences\n",
      "merge 23/256: (46, 271) -> 278 (b'.\\n\\n') had 5018 occurrences\n",
      "merge 24/256: (105, 259) -> 279 (b'is ') had 4913 occurrences\n",
      "merge 25/256: (101, 115) -> 280 (b'es') had 4878 occurrences\n",
      "merge 26/256: (121, 262) -> 281 (b'you') had 4649 occurrences\n",
      "merge 27/256: (32, 115) -> 282 (b' s') had 4207 occurrences\n",
      "merge 28/256: (116, 269) -> 283 (b'to ') had 4099 occurrences\n",
      "merge 29/256: (266, 260) -> 284 (b'and ') had 3968 occurrences\n",
      "merge 30/256: (111, 119) -> 285 (b'ow') had 3964 occurrences\n",
      "merge 31/256: (101, 97) -> 286 (b'ea') had 3943 occurrences\n",
      "merge 32/256: (32, 109) -> 287 (b' m') had 3906 occurrences\n",
      "merge 33/256: (32, 119) -> 288 (b' w') had 3854 occurrences\n",
      "merge 34/256: (111, 102) -> 289 (b'of') had 3836 occurrences\n",
      "merge 35/256: (32, 104) -> 290 (b' h') had 3668 occurrences\n",
      "merge 36/256: (264, 103) -> 291 (b'ing') had 3660 occurrences\n",
      "merge 37/256: (111, 109) -> 292 (b'om') had 3614 occurrences\n",
      "merge 38/256: (32, 97) -> 293 (b' a') had 3124 occurrences\n",
      "merge 39/256: (99, 104) -> 294 (b'ch') had 2974 occurrences\n",
      "merge 40/256: (257, 256) -> 295 (b'the ') had 2967 occurrences\n",
      "merge 41/256: (115, 116) -> 296 (b'st') had 2961 occurrences\n",
      "merge 42/256: (32, 98) -> 297 (b' b') had 2855 occurrences\n",
      "merge 43/256: (110, 111) -> 298 (b'no') had 2756 occurrences\n",
      "merge 44/256: (105, 114) -> 299 (b'ir') had 2700 occurrences\n",
      "merge 45/256: (102, 268) -> 300 (b'for') had 2698 occurrences\n",
      "merge 46/256: (118, 256) -> 301 (b've ') had 2650 occurrences\n",
      "merge 47/256: (101, 261) -> 302 (b'e, ') had 2591 occurrences\n",
      "merge 48/256: (105, 257) -> 303 (b'ith') had 2421 occurrences\n",
      "merge 49/256: (273, 256) -> 304 (b' the ') had 2397 occurrences\n",
      "merge 50/256: (115, 101) -> 305 (b'se') had 2373 occurrences\n",
      "merge 51/256: (108, 105) -> 306 (b'li') had 2358 occurrences\n",
      "merge 52/256: (84, 104) -> 307 (b'Th') had 2356 occurrences\n",
      "merge 53/256: (275, 32) -> 308 (b'll ') had 2246 occurrences\n",
      "merge 54/256: (114, 101) -> 309 (b're') had 2164 occurrences\n",
      "merge 55/256: (115, 258) -> 310 (b'st ') had 2125 occurrences\n",
      "merge 56/256: (97, 258) -> 311 (b'at ') had 2124 occurrences\n",
      "merge 57/256: (65, 110) -> 312 (b'An') had 2105 occurrences\n",
      "merge 58/256: (73, 32) -> 313 (b'I ') had 2092 occurrences\n",
      "merge 59/256: (101, 272) -> 314 (b'ear') had 2081 occurrences\n",
      "merge 60/256: (105, 109) -> 315 (b'im') had 2077 occurrences\n",
      "merge 61/256: (105, 116) -> 316 (b'it') had 2070 occurrences\n",
      "merge 62/256: (111, 111) -> 317 (b'oo') had 2025 occurrences\n",
      "merge 63/256: (103, 104) -> 318 (b'gh') had 1981 occurrences\n",
      "merge 64/256: (97, 116) -> 319 (b'at') had 1977 occurrences\n",
      "merge 65/256: (105, 115) -> 320 (b'is') had 1941 occurrences\n",
      "merge 66/256: (108, 101) -> 321 (b'le') had 1896 occurrences\n",
      "merge 67/256: (263, 32) -> 322 (b'er ') had 1847 occurrences\n",
      "merge 68/256: (262, 114) -> 323 (b'our') had 1816 occurrences\n",
      "merge 69/256: (312, 260) -> 324 (b'And ') had 1801 occurrences\n",
      "merge 70/256: (39, 259) -> 325 (b\"'s \") had 1767 occurrences\n",
      "merge 71/256: (101, 101) -> 326 (b'ee') had 1763 occurrences\n",
      "merge 72/256: (298, 258) -> 327 (b'not ') had 1749 occurrences\n",
      "merge 73/256: (109, 265) -> 328 (b'my ') had 1725 occurrences\n",
      "merge 74/256: (59, 10) -> 329 (b';\\n') had 1688 occurrences\n",
      "merge 75/256: (114, 97) -> 330 (b'ra') had 1667 occurrences\n",
      "merge 76/256: (46, 10) -> 331 (b'.\\n') had 1658 occurrences\n",
      "merge 77/256: (281, 114) -> 332 (b'your') had 1634 occurrences\n",
      "merge 78/256: (117, 114) -> 333 (b'ur') had 1632 occurrences\n",
      "merge 79/256: (276, 258) -> 334 (b'hat ') had 1562 occurrences\n",
      "merge 80/256: (114, 105) -> 335 (b'ri') had 1560 occurrences\n",
      "merge 81/256: (117, 258) -> 336 (b'ut ') had 1555 occurrences\n",
      "merge 82/256: (108, 260) -> 337 (b'ld ') had 1545 occurrences\n",
      "merge 83/256: (289, 32) -> 338 (b'of ') had 1494 occurrences\n",
      "merge 84/256: (79, 267) -> 339 (b'O:\\n') had 1494 occurrences\n",
      "merge 85/256: (101, 260) -> 340 (b'ed ') had 1479 occurrences\n",
      "merge 86/256: (108, 97) -> 341 (b'la') had 1460 occurrences\n",
      "merge 87/256: (105, 258) -> 342 (b'it ') had 1444 occurrences\n",
      "merge 88/256: (114, 111) -> 343 (b'ro') had 1434 occurrences\n",
      "merge 89/256: (263, 256) -> 344 (b'ere ') had 1397 occurrences\n",
      "merge 90/256: (101, 259) -> 345 (b'es ') had 1385 occurrences\n",
      "merge 91/256: (100, 261) -> 346 (b'd, ') had 1381 occurrences\n",
      "merge 92/256: (117, 110) -> 347 (b'un') had 1374 occurrences\n",
      "merge 93/256: (69, 78) -> 348 (b'EN') had 1373 occurrences\n",
      "merge 94/256: (107, 256) -> 349 (b'ke ') had 1367 occurrences\n",
      "merge 95/256: (121, 261) -> 350 (b'y, ') had 1339 occurrences\n",
      "merge 96/256: (73, 78) -> 351 (b'IN') had 1313 occurrences\n",
      "merge 97/256: (32, 100) -> 352 (b' d') had 1295 occurrences\n",
      "merge 98/256: (63, 271) -> 353 (b'?\\n\\n') had 1294 occurrences\n",
      "merge 99/256: (97, 259) -> 354 (b'as ') had 1294 occurrences\n",
      "merge 100/256: (102, 97) -> 355 (b'fa') had 1267 occurrences\n",
      "merge 101/256: (119, 303) -> 356 (b'with') had 1258 occurrences\n",
      "merge 102/256: (276, 301) -> 357 (b'have ') had 1240 occurrences\n",
      "merge 103/256: (83, 267) -> 358 (b'S:\\n') had 1230 occurrences\n",
      "merge 104/256: (32, 99) -> 359 (b' c') had 1228 occurrences\n",
      "merge 105/256: (87, 104) -> 360 (b'Wh') had 1226 occurrences\n",
      "merge 106/256: (257, 311) -> 361 (b'that ') had 1222 occurrences\n",
      "merge 107/256: (270, 116) -> 362 (b'ent') had 1221 occurrences\n",
      "merge 108/256: (257, 101) -> 363 (b'the') had 1213 occurrences\n",
      "merge 109/256: (99, 101) -> 364 (b'ce') had 1206 occurrences\n",
      "merge 110/256: (115, 104) -> 365 (b'sh') had 1195 occurrences\n",
      "merge 111/256: (109, 97) -> 366 (b'ma') had 1173 occurrences\n",
      "merge 112/256: (32, 112) -> 367 (b' p') had 1167 occurrences\n",
      "merge 113/256: (257, 263) -> 368 (b'ther') had 1133 occurrences\n",
      "merge 114/256: (98, 101) -> 369 (b'be') had 1131 occurrences\n",
      "merge 115/256: (46, 32) -> 370 (b'. ') had 1127 occurrences\n",
      "merge 116/256: (65, 82) -> 371 (b'AR') had 1124 occurrences\n",
      "merge 117/256: (99, 256) -> 372 (b'ce ') had 1116 occurrences\n",
      "merge 118/256: (291, 32) -> 373 (b'ing ') had 1113 occurrences\n",
      "merge 119/256: (97, 108) -> 374 (b'al') had 1098 occurrences\n",
      "merge 120/256: (59, 32) -> 375 (b'; ') had 1091 occurrences\n",
      "merge 121/256: (257, 262) -> 376 (b'thou') had 1088 occurrences\n",
      "merge 122/256: (115, 261) -> 377 (b's, ') had 1086 occurrences\n",
      "merge 123/256: (109, 256) -> 378 (b'me ') had 1081 occurrences\n",
      "merge 124/256: (115, 256) -> 379 (b'se ') had 1078 occurrences\n",
      "merge 125/256: (108, 111) -> 380 (b'lo') had 1077 occurrences\n",
      "merge 126/256: (99, 107) -> 381 (b'ck') had 1061 occurrences\n",
      "merge 127/256: (119, 104) -> 382 (b'wh') had 1057 occurrences\n",
      "merge 128/256: (105, 108) -> 383 (b'il') had 1046 occurrences\n",
      "merge 129/256: (39, 260) -> 384 (b\"'d \") had 1026 occurrences\n",
      "merge 130/256: (73, 339) -> 385 (b'IO:\\n') had 1025 occurrences\n",
      "merge 131/256: (110, 285) -> 386 (b'now') had 1022 occurrences\n",
      "merge 132/256: (105, 275) -> 387 (b'ill') had 1016 occurrences\n",
      "merge 133/256: (98, 256) -> 388 (b'be ') had 982 occurrences\n",
      "merge 134/256: (101, 275) -> 389 (b'ell') had 982 occurrences\n",
      "merge 135/256: (114, 286) -> 390 (b'rea') had 978 occurrences\n",
      "merge 136/256: (32, 116) -> 391 (b' t') had 972 occurrences\n",
      "merge 137/256: (116, 261) -> 392 (b't, ') had 971 occurrences\n",
      "merge 138/256: (262, 337) -> 393 (b'ould ') had 970 occurrences\n",
      "merge 139/256: (101, 10) -> 394 (b'e\\n') had 962 occurrences\n",
      "merge 140/256: (287, 265) -> 395 (b' my ') had 959 occurrences\n",
      "merge 141/256: (118, 263) -> 396 (b'ver') had 955 occurrences\n",
      "merge 142/256: (99, 292) -> 397 (b'com') had 952 occurrences\n",
      "merge 143/256: (104, 256) -> 398 (b'he ') had 929 occurrences\n",
      "merge 144/256: (32, 283) -> 399 (b' to ') had 926 occurrences\n",
      "merge 145/256: (32, 73) -> 400 (b' I') had 906 occurrences\n",
      "merge 146/256: (101, 108) -> 401 (b'el') had 902 occurrences\n",
      "merge 147/256: (85, 358) -> 402 (b'US:\\n') had 879 occurrences\n",
      "merge 148/256: (111, 108) -> 403 (b'ol') had 871 occurrences\n",
      "merge 149/256: (100, 105) -> 404 (b'di') had 869 occurrences\n",
      "merge 150/256: (32, 103) -> 405 (b' g') had 864 occurrences\n",
      "merge 151/256: (97, 265) -> 406 (b'ay ') had 849 occurrences\n",
      "merge 152/256: (116, 263) -> 407 (b'ter') had 849 occurrences\n",
      "merge 153/256: (97, 264) -> 408 (b'ain') had 844 occurrences\n",
      "merge 154/256: (32, 281) -> 409 (b' you') had 844 occurrences\n",
      "merge 155/256: (307, 256) -> 410 (b'The ') had 843 occurrences\n",
      "merge 156/256: (108, 256) -> 411 (b'le ') had 839 occurrences\n",
      "merge 157/256: (105, 274) -> 412 (b'ion') had 838 occurrences\n",
      "merge 158/256: (32, 102) -> 413 (b' f') had 826 occurrences\n",
      "merge 159/256: (114, 117) -> 414 (b'ru') had 819 occurrences\n",
      "merge 160/256: (105, 102) -> 415 (b'if') had 817 occurrences\n",
      "merge 161/256: (101, 109) -> 416 (b'em') had 810 occurrences\n",
      "merge 162/256: (266, 100) -> 417 (b'and') had 801 occurrences\n",
      "merge 163/256: (84, 269) -> 418 (b'To ') had 800 occurrences\n",
      "merge 164/256: (105, 318) -> 419 (b'igh') had 798 occurrences\n",
      "merge 165/256: (272, 256) -> 420 (b'are ') had 796 occurrences\n",
      "merge 166/256: (117, 112) -> 421 (b'up') had 779 occurrences\n",
      "merge 167/256: (277, 324) -> 422 (b',\\nAnd ') had 774 occurrences\n",
      "merge 168/256: (104, 315) -> 423 (b'him') had 761 occurrences\n",
      "merge 169/256: (101, 100) -> 424 (b'ed') had 751 occurrences\n",
      "merge 170/256: (105, 308) -> 425 (b'ill ') had 743 occurrences\n",
      "merge 171/256: (268, 100) -> 426 (b'ord') had 736 occurrences\n",
      "merge 172/256: (105, 294) -> 427 (b'ich') had 733 occurrences\n",
      "merge 173/256: (108, 265) -> 428 (b'ly ') had 732 occurrences\n",
      "merge 174/256: (317, 260) -> 429 (b'ood ') had 726 occurrences\n",
      "merge 175/256: (85, 67) -> 430 (b'UC') had 725 occurrences\n",
      "merge 176/256: (285, 110) -> 431 (b'own') had 717 occurrences\n",
      "merge 177/256: (104, 279) -> 432 (b'his ') had 706 occurrences\n",
      "merge 178/256: (351, 71) -> 433 (b'ING') had 703 occurrences\n",
      "merge 179/256: (32, 284) -> 434 (b' and ') had 701 occurrences\n",
      "merge 180/256: (99, 274) -> 435 (b'con') had 700 occurrences\n",
      "merge 181/256: (110, 101) -> 436 (b'ne') had 699 occurrences\n",
      "merge 182/256: (97, 121) -> 437 (b'ay') had 697 occurrences\n",
      "merge 183/256: (101, 278) -> 438 (b'e.\\n\\n') had 693 occurrences\n",
      "merge 184/256: (114, 292) -> 439 (b'rom') had 690 occurrences\n",
      "merge 185/256: (105, 100) -> 440 (b'id') had 681 occurrences\n",
      "merge 186/256: (117, 115) -> 441 (b'us') had 679 occurrences\n",
      "merge 187/256: (262, 110) -> 442 (b'oun') had 677 occurrences\n",
      "merge 188/256: (65, 78) -> 443 (b'AN') had 677 occurrences\n",
      "merge 189/256: (109, 266) -> 444 (b'man') had 675 occurrences\n",
      "merge 190/256: (97, 103) -> 445 (b'ag') had 669 occurrences\n",
      "merge 191/256: (69, 82) -> 446 (b'ER') had 665 occurrences\n",
      "merge 192/256: (79, 82) -> 447 (b'OR') had 663 occurrences\n",
      "merge 193/256: (101, 258) -> 448 (b'et ') had 657 occurrences\n",
      "merge 194/256: (114, 280) -> 449 (b'res') had 655 occurrences\n",
      "merge 195/256: (305, 108) -> 450 (b'sel') had 649 occurrences\n",
      "merge 196/256: (290, 279) -> 451 (b' his ') had 647 occurrences\n",
      "merge 197/256: (101, 277) -> 452 (b'e,\\n') had 646 occurrences\n",
      "merge 198/256: (101, 116) -> 453 (b'et') had 643 occurrences\n",
      "merge 199/256: (99, 97) -> 454 (b'ca') had 642 occurrences\n",
      "merge 200/256: (32, 264) -> 455 (b' in') had 641 occurrences\n",
      "merge 201/256: (115, 276) -> 456 (b'sha') had 636 occurrences\n",
      "merge 202/256: (33, 10) -> 457 (b'!\\n') had 635 occurrences\n",
      "merge 203/256: (69, 84) -> 458 (b'ET') had 628 occurrences\n",
      "merge 204/256: (84, 334) -> 459 (b'That ') had 615 occurrences\n",
      "merge 205/256: (112, 111) -> 460 (b'po') had 611 occurrences\n",
      "merge 206/256: (113, 117) -> 461 (b'qu') had 609 occurrences\n",
      "merge 207/256: (257, 265) -> 462 (b'thy ') had 596 occurrences\n",
      "merge 208/256: (33, 271) -> 463 (b'!\\n\\n') had 594 occurrences\n",
      "merge 209/256: (109, 268) -> 464 (b'mor') had 584 occurrences\n",
      "merge 210/256: (117, 108) -> 465 (b'ul') had 581 occurrences\n",
      "merge 211/256: (110, 269) -> 466 (b'no ') had 579 occurrences\n",
      "merge 212/256: (97, 109) -> 467 (b'am') had 577 occurrences\n",
      "merge 213/256: (273, 101) -> 468 (b' the') had 572 occurrences\n",
      "merge 214/256: (65, 267) -> 469 (b'A:\\n') had 570 occurrences\n",
      "merge 215/256: (118, 270) -> 470 (b'ven') had 569 occurrences\n",
      "merge 216/256: (98, 265) -> 471 (b'by ') had 566 occurrences\n",
      "merge 217/256: (115, 10) -> 472 (b's\\n') had 560 occurrences\n",
      "merge 218/256: (115, 112) -> 473 (b'sp') had 556 occurrences\n",
      "merge 219/256: (75, 433) -> 474 (b'KING') had 556 occurrences\n",
      "merge 220/256: (290, 315) -> 475 (b' him') had 553 occurrences\n",
      "merge 221/256: (257, 279) -> 476 (b'this ') had 552 occurrences\n",
      "merge 222/256: (273, 279) -> 477 (b' this ') had 552 occurrences\n",
      "merge 223/256: (104, 263) -> 478 (b'her') had 552 occurrences\n",
      "merge 224/256: (273, 311) -> 479 (b' that ') had 542 occurrences\n",
      "merge 225/256: (111, 257) -> 480 (b'oth') had 539 occurrences\n",
      "merge 226/256: (63, 10) -> 481 (b'?\\n') had 539 occurrences\n",
      "merge 227/256: (274, 103) -> 482 (b'ong') had 537 occurrences\n",
      "merge 228/256: (66, 336) -> 483 (b'But ') had 536 occurrences\n",
      "merge 229/256: (280, 258) -> 484 (b'est ') had 532 occurrences\n",
      "merge 230/256: (111, 261) -> 485 (b'o, ') had 528 occurrences\n",
      "merge 231/256: (98, 336) -> 486 (b'but ') had 526 occurrences\n",
      "merge 232/256: (32, 289) -> 487 (b' of') had 525 occurrences\n",
      "merge 233/256: (70, 268) -> 488 (b'For') had 522 occurrences\n",
      "merge 234/256: (115, 117) -> 489 (b'su') had 521 occurrences\n",
      "merge 235/256: (288, 303) -> 490 (b' with') had 517 occurrences\n",
      "merge 236/256: (117, 116) -> 491 (b'ut') had 517 occurrences\n",
      "merge 237/256: (274, 256) -> 492 (b'one ') had 514 occurrences\n",
      "merge 238/256: (97, 275) -> 493 (b'all') had 512 occurrences\n",
      "merge 239/256: (73, 67) -> 494 (b'IC') had 509 occurrences\n",
      "merge 240/256: (270, 100) -> 495 (b'end') had 500 occurrences\n",
      "merge 241/256: (79, 76) -> 496 (b'OL') had 497 occurrences\n",
      "merge 242/256: (100, 269) -> 497 (b'do ') had 495 occurrences\n",
      "merge 243/256: (73, 288) -> 498 (b'I w') had 495 occurrences\n",
      "merge 244/256: (292, 256) -> 499 (b'ome ') had 494 occurrences\n",
      "merge 245/256: (107, 386) -> 500 (b'know') had 493 occurrences\n",
      "merge 246/256: (115, 277) -> 501 (b's,\\n') had 493 occurrences\n",
      "merge 247/256: (115, 299) -> 502 (b'sir') had 493 occurrences\n",
      "merge 248/256: (261, 284) -> 503 (b', and ') had 490 occurrences\n",
      "merge 249/256: (99, 116) -> 504 (b'ct') had 490 occurrences\n",
      "merge 250/256: (117, 259) -> 505 (b'us ') had 489 occurrences\n",
      "merge 251/256: (280, 259) -> 506 (b'ess ') had 486 occurrences\n",
      "merge 252/256: (450, 102) -> 507 (b'self') had 486 occurrences\n",
      "merge 253/256: (282, 116) -> 508 (b' st') had 486 occurrences\n",
      "merge 254/256: (97, 349) -> 509 (b'ake ') had 481 occurrences\n",
      "merge 255/256: (69, 76) -> 510 (b'EL') had 481 occurrences\n",
      "merge 256/256: (107, 291) -> 511 (b'king') had 479 occurrences\n",
      "merge 1/256: (32, 116) -> 256 (b' t') had 23837 occurrences\n",
      "merge 2/256: (104, 101) -> 257 (b'he') had 18203 occurrences\n",
      "merge 3/256: (32, 97) -> 258 (b' a') had 13541 occurrences\n",
      "merge 4/256: (111, 117) -> 259 (b'ou') had 12730 occurrences\n",
      "merge 5/256: (32, 115) -> 260 (b' s') had 12287 occurrences\n",
      "merge 6/256: (32, 109) -> 261 (b' m') had 10786 occurrences\n",
      "merge 7/256: (105, 110) -> 262 (b'in') had 10606 occurrences\n",
      "merge 8/256: (32, 119) -> 263 (b' w') had 10546 occurrences\n",
      "merge 9/256: (114, 101) -> 264 (b're') had 9843 occurrences\n",
      "merge 10/256: (104, 97) -> 265 (b'ha') had 9673 occurrences\n",
      "merge 11/256: (58, 10) -> 266 (b':\\n') had 8762 occurrences\n",
      "merge 12/256: (110, 100) -> 267 (b'nd') had 8730 occurrences\n",
      "merge 13/256: (256, 257) -> 268 (b' the') had 8684 occurrences\n",
      "merge 14/256: (32, 98) -> 269 (b' b') had 8463 occurrences\n",
      "merge 15/256: (105, 115) -> 270 (b'is') had 7526 occurrences\n",
      "merge 16/256: (111, 114) -> 271 (b'or') had 7297 occurrences\n",
      "merge 17/256: (10, 10) -> 272 (b'\\n\\n') had 7098 occurrences\n",
      "merge 18/256: (32, 102) -> 273 (b' f') had 6563 occurrences\n",
      "merge 19/256: (101, 114) -> 274 (b'er') had 6515 occurrences\n",
      "merge 20/256: (108, 108) -> 275 (b'll') had 6357 occurrences\n",
      "merge 21/256: (105, 116) -> 276 (b'it') had 6114 occurrences\n",
      "merge 22/256: (111, 110) -> 277 (b'on') had 5973 occurrences\n",
      "merge 23/256: (44, 10) -> 278 (b',\\n') had 5501 occurrences\n",
      "merge 24/256: (32, 100) -> 279 (b' d') had 5478 occurrences\n",
      "merge 25/256: (32, 99) -> 280 (b' c') had 5404 occurrences\n",
      "merge 26/256: (101, 115) -> 281 (b'es') had 5202 occurrences\n",
      "merge 27/256: (101, 110) -> 282 (b'en') had 5181 occurrences\n",
      "merge 28/256: (32, 110) -> 283 (b' n') had 5176 occurrences\n",
      "merge 29/256: (32, 108) -> 284 (b' l') had 5173 occurrences\n",
      "merge 30/256: (32, 121) -> 285 (b' y') had 5140 occurrences\n",
      "merge 31/256: (46, 272) -> 286 (b'.\\n\\n') had 5018 occurrences\n",
      "merge 32/256: (256, 104) -> 287 (b' th') had 4940 occurrences\n",
      "merge 33/256: (97, 114) -> 288 (b'ar') had 4884 occurrences\n",
      "merge 34/256: (32, 104) -> 289 (b' h') had 4702 occurrences\n",
      "merge 35/256: (32, 111) -> 290 (b' o') had 4693 occurrences\n",
      "merge 36/256: (256, 111) -> 291 (b' to') had 4666 occurrences\n",
      "merge 37/256: (285, 259) -> 292 (b' you') had 4587 occurrences\n",
      "merge 38/256: (32, 112) -> 293 (b' p') had 4490 occurrences\n",
      "merge 39/256: (265, 116) -> 294 (b'hat') had 4407 occurrences\n",
      "merge 40/256: (32, 73) -> 295 (b' I') had 4079 occurrences\n",
      "merge 41/256: (32, 257) -> 296 (b' he') had 4022 occurrences\n",
      "merge 42/256: (118, 101) -> 297 (b've') had 3900 occurrences\n",
      "merge 43/256: (111, 116) -> 298 (b'ot') had 3891 occurrences\n",
      "merge 44/256: (115, 116) -> 299 (b'st') had 3709 occurrences\n",
      "merge 45/256: (258, 267) -> 300 (b' and') had 3703 occurrences\n",
      "merge 46/256: (111, 119) -> 301 (b'ow') had 3686 occurrences\n",
      "merge 47/256: (262, 103) -> 302 (b'ing') had 3660 occurrences\n",
      "merge 48/256: (97, 110) -> 303 (b'an') had 3635 occurrences\n",
      "merge 49/256: (290, 102) -> 304 (b' of') had 3605 occurrences\n",
      "merge 50/256: (111, 109) -> 305 (b'om') had 3584 occurrences\n",
      "merge 51/256: (32, 103) -> 306 (b' g') had 3512 occurrences\n",
      "merge 52/256: (97, 116) -> 307 (b'at') had 3357 occurrences\n",
      "merge 53/256: (269, 101) -> 308 (b' be') had 3219 occurrences\n",
      "merge 54/256: (115, 101) -> 309 (b'se') had 3065 occurrences\n",
      "merge 55/256: (261, 121) -> 310 (b' my') had 2829 occurrences\n",
      "merge 56/256: (32, 262) -> 311 (b' in') had 2746 occurrences\n",
      "merge 57/256: (99, 101) -> 312 (b'ce') had 2740 occurrences\n",
      "merge 58/256: (32, 265) -> 313 (b' ha') had 2713 occurrences\n",
      "merge 59/256: (108, 101) -> 314 (b'le') had 2636 occurrences\n",
      "merge 60/256: (97, 121) -> 315 (b'ay') had 2568 occurrences\n",
      "merge 61/256: (108, 100) -> 316 (b'ld') had 2392 occurrences\n",
      "merge 62/256: (105, 114) -> 317 (b'ir') had 2385 occurrences\n",
      "merge 63/256: (101, 116) -> 318 (b'et') had 2375 occurrences\n",
      "merge 64/256: (101, 100) -> 319 (b'ed') had 2344 occurrences\n",
      "merge 65/256: (117, 116) -> 320 (b'ut') had 2304 occurrences\n",
      "merge 66/256: (261, 101) -> 321 (b' me') had 2132 occurrences\n",
      "merge 67/256: (105, 109) -> 322 (b'im') had 2125 occurrences\n",
      "merge 68/256: (276, 104) -> 323 (b'ith') had 2089 occurrences\n",
      "merge 69/256: (39, 115) -> 324 (b\"'s\") had 2063 occurrences\n",
      "merge 70/256: (283, 298) -> 325 (b' not') had 2048 occurrences\n",
      "merge 71/256: (99, 104) -> 326 (b'ch') had 2016 occurrences\n",
      "merge 72/256: (256, 294) -> 327 (b' that') had 1962 occurrences\n",
      "merge 73/256: (32, 270) -> 328 (b' is') had 1955 occurrences\n",
      "merge 74/256: (103, 104) -> 329 (b'gh') had 1947 occurrences\n",
      "merge 75/256: (65, 267) -> 330 (b'And') had 1927 occurrences\n",
      "merge 76/256: (273, 271) -> 331 (b' for') had 1890 occurrences\n",
      "merge 77/256: (107, 101) -> 332 (b'ke') had 1864 occurrences\n",
      "merge 78/256: (32, 117) -> 333 (b' u') had 1850 occurrences\n",
      "merge 79/256: (259, 114) -> 334 (b'our') had 1837 occurrences\n",
      "merge 80/256: (263, 101) -> 335 (b' we') had 1816 occurrences\n",
      "merge 81/256: (111, 111) -> 336 (b'oo') had 1800 occurrences\n",
      "merge 82/256: (105, 275) -> 337 (b'ill') had 1763 occurrences\n",
      "merge 83/256: (32, 101) -> 338 (b' e') had 1733 occurrences\n",
      "merge 84/256: (257, 114) -> 339 (b'her') had 1710 occurrences\n",
      "merge 85/256: (59, 10) -> 340 (b';\\n') had 1688 occurrences\n",
      "merge 86/256: (263, 323) -> 341 (b' with') had 1676 occurrences\n",
      "merge 87/256: (46, 10) -> 342 (b'.\\n') had 1658 occurrences\n",
      "merge 88/256: (282, 116) -> 343 (b'ent') had 1638 occurrences\n",
      "merge 89/256: (32, 276) -> 344 (b' it') had 1627 occurrences\n",
      "merge 90/256: (292, 114) -> 345 (b' your') had 1610 occurrences\n",
      "merge 91/256: (97, 100) -> 346 (b'ad') had 1598 occurrences\n",
      "merge 92/256: (114, 105) -> 347 (b'ri') had 1545 occurrences\n",
      "merge 93/256: (287, 259) -> 348 (b' thou') had 1496 occurrences\n",
      "merge 94/256: (260, 116) -> 349 (b' st') had 1478 occurrences\n",
      "merge 95/256: (39, 100) -> 350 (b\"'d\") had 1451 occurrences\n",
      "merge 96/256: (32, 107) -> 351 (b' k') had 1438 occurrences\n",
      "merge 97/256: (305, 101) -> 352 (b'ome') had 1436 occurrences\n",
      "merge 98/256: (289, 270) -> 353 (b' his') had 1415 occurrences\n",
      "merge 99/256: (329, 116) -> 354 (b'ght') had 1379 occurrences\n",
      "merge 100/256: (69, 78) -> 355 (b'EN') had 1373 occurrences\n",
      "merge 101/256: (271, 100) -> 356 (b'ord') had 1353 occurrences\n",
      "merge 102/256: (105, 100) -> 357 (b'id') had 1350 occurrences\n",
      "merge 103/256: (97, 115) -> 358 (b'as') had 1347 occurrences\n",
      "merge 104/256: (84, 257) -> 359 (b'The') had 1345 occurrences\n",
      "merge 105/256: (32, 264) -> 360 (b' re') had 1330 occurrences\n",
      "merge 106/256: (313, 297) -> 361 (b' have') had 1325 occurrences\n",
      "merge 107/256: (73, 78) -> 362 (b'IN') had 1313 occurrences\n",
      "merge 108/256: (108, 121) -> 363 (b'ly') had 1312 occurrences\n",
      "merge 109/256: (114, 97) -> 364 (b'ra') had 1303 occurrences\n",
      "merge 110/256: (284, 105) -> 365 (b' li') had 1299 occurrences\n",
      "merge 111/256: (63, 272) -> 366 (b'?\\n\\n') had 1294 occurrences\n",
      "merge 112/256: (289, 322) -> 367 (b' him') had 1293 occurrences\n",
      "merge 113/256: (117, 114) -> 368 (b'ur') had 1263 occurrences\n",
      "merge 114/256: (287, 270) -> 369 (b' this') had 1261 occurrences\n",
      "merge 115/256: (97, 108) -> 370 (b'al') had 1256 occurrences\n",
      "merge 116/256: (73, 79) -> 371 (b'IO') had 1254 occurrences\n",
      "merge 117/256: (260, 111) -> 372 (b' so') had 1238 occurrences\n",
      "merge 118/256: (258, 115) -> 373 (b' as') had 1209 occurrences\n",
      "merge 119/256: (279, 101) -> 374 (b' de') had 1202 occurrences\n",
      "merge 120/256: (32, 277) -> 375 (b' on') had 1178 occurrences\n",
      "merge 121/256: (111, 264) -> 376 (b'ore') had 1147 occurrences\n",
      "merge 122/256: (114, 111) -> 377 (b'ro') had 1127 occurrences\n",
      "merge 123/256: (65, 82) -> 378 (b'AR') had 1124 occurrences\n",
      "merge 124/256: (104, 105) -> 379 (b'hi') had 1120 occurrences\n",
      "merge 125/256: (259, 316) -> 380 (b'ould') had 1098 occurrences\n",
      "merge 126/256: (336, 100) -> 381 (b'ood') had 1092 occurrences\n",
      "merge 127/256: (99, 107) -> 382 (b'ck') had 1056 occurrences\n",
      "merge 128/256: (97, 262) -> 383 (b'ain') had 1051 occurrences\n",
      "merge 129/256: (118, 274) -> 384 (b'ver') had 1042 occurrences\n",
      "merge 130/256: (281, 116) -> 385 (b'est') had 1008 occurrences\n",
      "merge 131/256: (287, 121) -> 386 (b' thy') had 994 occurrences\n",
      "merge 132/256: (260, 265) -> 387 (b' sha') had 993 occurrences\n",
      "merge 133/256: (281, 115) -> 388 (b'ess') had 990 occurrences\n",
      "merge 134/256: (101, 97) -> 389 (b'ea') had 972 occurrences\n",
      "merge 135/256: (279, 111) -> 390 (b' do') had 968 occurrences\n",
      "merge 136/256: (263, 337) -> 391 (b' will') had 966 occurrences\n",
      "merge 137/256: (97, 109) -> 392 (b'am') had 954 occurrences\n",
      "merge 138/256: (283, 111) -> 393 (b' no') had 943 occurrences\n",
      "merge 139/256: (269, 320) -> 394 (b' but') had 912 occurrences\n",
      "merge 140/256: (117, 115) -> 395 (b'us') had 907 occurrences\n",
      "merge 141/256: (97, 267) -> 396 (b'and') had 897 occurrences\n",
      "merge 142/256: (85, 83) -> 397 (b'US') had 895 occurrences\n",
      "merge 143/256: (105, 102) -> 398 (b'if') had 894 occurrences\n",
      "merge 144/256: (260, 101) -> 399 (b' se') had 881 occurrences\n",
      "merge 145/256: (103, 101) -> 400 (b'ge') had 877 occurrences\n",
      "merge 146/256: (258, 275) -> 401 (b' all') had 849 occurrences\n",
      "merge 147/256: (84, 104) -> 402 (b'Th') had 845 occurrences\n",
      "merge 148/256: (260, 117) -> 403 (b' su') had 830 occurrences\n",
      "merge 149/256: (97, 332) -> 404 (b'ake') had 830 occurrences\n",
      "merge 150/256: (84, 111) -> 405 (b'To') had 828 occurrences\n",
      "merge 151/256: (296, 114) -> 406 (b' her') had 811 occurrences\n",
      "merge 152/256: (114, 117) -> 407 (b'ru') had 810 occurrences\n",
      "merge 153/256: (105, 277) -> 408 (b'ion') had 808 occurrences\n",
      "merge 154/256: (116, 104) -> 409 (b'th') had 800 occurrences\n",
      "merge 155/256: (258, 110) -> 410 (b' an') had 789 occurrences\n",
      "merge 156/256: (116, 274) -> 411 (b'ter') had 786 occurrences\n",
      "merge 157/256: (288, 100) -> 412 (b'ard') had 786 occurrences\n",
      "merge 158/256: (284, 111) -> 413 (b' lo') had 782 occurrences\n",
      "merge 159/256: (265, 110) -> 414 (b'han') had 779 occurrences\n",
      "merge 160/256: (101, 275) -> 415 (b'ell') had 771 occurrences\n",
      "merge 161/256: (101, 288) -> 416 (b'ear') had 769 occurrences\n",
      "merge 162/256: (260, 112) -> 417 (b' sp') had 763 occurrences\n",
      "merge 163/256: (268, 101) -> 418 (b' thee') had 751 occurrences\n",
      "merge 164/256: (32, 334) -> 419 (b' our') had 742 occurrences\n",
      "merge 165/256: (273, 97) -> 420 (b' fa') had 740 occurrences\n",
      "merge 166/256: (387, 275) -> 421 (b' shall') had 740 occurrences\n",
      "merge 167/256: (269, 121) -> 422 (b' by') had 739 occurrences\n",
      "merge 168/256: (85, 67) -> 423 (b'UC') had 725 occurrences\n",
      "merge 169/256: (105, 108) -> 424 (b'il') had 709 occurrences\n",
      "merge 170/256: (258, 264) -> 425 (b' are') had 703 occurrences\n",
      "merge 171/256: (362, 71) -> 426 (b'ING') had 703 occurrences\n",
      "merge 172/256: (32, 67) -> 427 (b' C') had 693 occurrences\n",
      "merge 173/256: (283, 101) -> 428 (b' ne') had 690 occurrences\n",
      "merge 174/256: (114, 305) -> 429 (b'rom') had 690 occurrences\n",
      "merge 175/256: (104, 111) -> 430 (b'ho') had 686 occurrences\n",
      "merge 176/256: (351, 110) -> 431 (b' kn') had 683 occurrences\n",
      "merge 177/256: (65, 78) -> 432 (b'AN') had 677 occurrences\n",
      "merge 178/256: (32, 82) -> 433 (b' R') had 675 occurrences\n",
      "merge 179/256: (84, 294) -> 434 (b'That') had 671 occurrences\n",
      "merge 180/256: (32, 118) -> 435 (b' v') had 670 occurrences\n",
      "merge 181/256: (69, 82) -> 436 (b'ER') had 665 occurrences\n",
      "merge 182/256: (97, 299) -> 437 (b'ast') had 663 occurrences\n",
      "merge 183/256: (79, 82) -> 438 (b'OR') had 663 occurrences\n",
      "merge 184/256: (99, 116) -> 439 (b'ct') had 661 occurrences\n",
      "merge 185/256: (259, 115) -> 440 (b'ous') had 661 occurrences\n",
      "merge 186/256: (263, 294) -> 441 (b' what') had 655 occurrences\n",
      "merge 187/256: (105, 354) -> 442 (b'ight') had 642 occurrences\n",
      "merge 188/256: (260, 104) -> 443 (b' sh') had 639 occurrences\n",
      "merge 189/256: (33, 10) -> 444 (b'!\\n') had 635 occurrences\n",
      "merge 190/256: (117, 108) -> 445 (b'ul') had 633 occurrences\n",
      "merge 191/256: (32, 39) -> 446 (b\" '\") had 628 occurrences\n",
      "merge 192/256: (69, 84) -> 447 (b'ET') had 628 occurrences\n",
      "merge 193/256: (303, 116) -> 448 (b'ant') had 627 occurrences\n",
      "merge 194/256: (69, 83) -> 449 (b'ES') had 624 occurrences\n",
      "merge 195/256: (333, 112) -> 450 (b' up') had 616 occurrences\n",
      "merge 196/256: (309, 108) -> 451 (b'sel') had 614 occurrences\n",
      "merge 197/256: (113, 117) -> 452 (b'qu') had 609 occurrences\n",
      "merge 198/256: (66, 320) -> 453 (b'But') had 608 occurrences\n",
      "merge 199/256: (288, 116) -> 454 (b'art') had 607 occurrences\n",
      "merge 200/256: (306, 381) -> 455 (b' good') had 599 occurrences\n",
      "merge 201/256: (33, 272) -> 456 (b'!\\n\\n') had 594 occurrences\n",
      "merge 202/256: (114, 301) -> 457 (b'row') had 592 occurrences\n",
      "merge 203/256: (307, 104) -> 458 (b'ath') had 591 occurrences\n",
      "merge 204/256: (262, 101) -> 459 (b'ine') had 591 occurrences\n",
      "merge 205/256: (284, 356) -> 460 (b' lord') had 589 occurrences\n",
      "merge 206/256: (379, 326) -> 461 (b'hich') had 587 occurrences\n",
      "merge 207/256: (110, 116) -> 462 (b'nt') had 586 occurrences\n",
      "merge 208/256: (117, 299) -> 463 (b'ust') had 585 occurrences\n",
      "merge 209/256: (39, 275) -> 464 (b\"'ll\") had 580 occurrences\n",
      "merge 210/256: (277, 101) -> 465 (b'one') had 578 occurrences\n",
      "merge 211/256: (293, 114) -> 466 (b' pr') had 577 occurrences\n",
      "merge 212/256: (280, 305) -> 467 (b' com') had 575 occurrences\n",
      "merge 213/256: (258, 116) -> 468 (b' at') had 574 occurrences\n",
      "merge 214/256: (261, 303) -> 469 (b' man') had 569 occurrences\n",
      "merge 215/256: (32, 77) -> 470 (b' M') had 567 occurrences\n",
      "merge 216/256: (87, 294) -> 471 (b'What') had 567 occurrences\n",
      "merge 217/256: (263, 257) -> 472 (b' whe') had 563 occurrences\n",
      "merge 218/256: (32, 69) -> 473 (b' E') had 559 occurrences\n",
      "merge 219/256: (75, 426) -> 474 (b'KING') had 556 occurrences\n",
      "merge 220/256: (258, 109) -> 475 (b' am') had 554 occurrences\n",
      "merge 221/256: (101, 267) -> 476 (b'end') had 553 occurrences\n",
      "merge 222/256: (105, 99) -> 477 (b'ic') had 548 occurrences\n",
      "merge 223/256: (280, 277) -> 478 (b' con') had 548 occurrences\n",
      "merge 224/256: (98, 314) -> 479 (b'ble') had 545 occurrences\n",
      "merge 225/256: (114, 121) -> 480 (b'ry') had 542 occurrences\n",
      "merge 226/256: (63, 10) -> 481 (b'?\\n') had 539 occurrences\n",
      "merge 227/256: (277, 103) -> 482 (b'ong') had 534 occurrences\n",
      "merge 228/256: (105, 101) -> 483 (b'ie') had 533 occurrences\n",
      "merge 229/256: (105, 297) -> 484 (b'ive') had 527 occurrences\n",
      "merge 230/256: (273, 429) -> 485 (b' from') had 527 occurrences\n",
      "merge 231/256: (269, 108) -> 486 (b' bl') had 527 occurrences\n",
      "merge 232/256: (118, 282) -> 487 (b'ven') had 521 occurrences\n",
      "merge 233/256: (32, 71) -> 488 (b' G') had 518 occurrences\n",
      "merge 234/256: (70, 271) -> 489 (b'For') had 517 occurrences\n",
      "merge 235/256: (260, 257) -> 490 (b' she') had 517 occurrences\n",
      "merge 236/256: (101, 109) -> 491 (b'em') had 515 occurrences\n",
      "merge 237/256: (306, 111) -> 492 (b' go') had 515 occurrences\n",
      "merge 238/256: (97, 264) -> 493 (b'are') had 514 occurrences\n",
      "merge 239/256: (261, 376) -> 494 (b' more') had 510 occurrences\n",
      "merge 240/256: (268, 109) -> 495 (b' them') had 509 occurrences\n",
      "merge 241/256: (259, 116) -> 496 (b'out') had 509 occurrences\n",
      "merge 242/256: (73, 67) -> 497 (b'IC') had 509 occurrences\n",
      "merge 243/256: (263, 358) -> 498 (b' was') had 504 occurrences\n",
      "merge 244/256: (97, 117) -> 499 (b'au') had 504 occurrences\n",
      "merge 245/256: (298, 339) -> 500 (b'other') had 503 occurrences\n",
      "merge 246/256: (298, 104) -> 501 (b'oth') had 503 occurrences\n",
      "merge 247/256: (72, 101) -> 502 (b'He') had 500 occurrences\n",
      "merge 248/256: (260, 317) -> 503 (b' sir') had 500 occurrences\n",
      "merge 249/256: (111, 108) -> 504 (b'ol') had 498 occurrences\n",
      "merge 250/256: (283, 301) -> 505 (b' now') had 490 occurrences\n",
      "merge 251/256: (32, 76) -> 506 (b' L') had 489 occurrences\n",
      "merge 252/256: (32, 294) -> 507 (b' hat') had 488 occurrences\n",
      "merge 253/256: (32, 398) -> 508 (b' if') had 484 occurrences\n",
      "merge 254/256: (111, 299) -> 509 (b'ost') had 484 occurrences\n",
      "merge 255/256: (76, 79) -> 510 (b'LO') had 481 occurrences\n",
      "merge 256/256: (262, 100) -> 511 (b'ind') had 480 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Test the tokeniser implementation\n",
    "from tokeniser import BasicTokeniser, RegexTokeniser\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "print(text[:150])\n",
    "\n",
    "tokeniser = BasicTokeniser()\n",
    "tokeniserregex = RegexTokeniser()\n",
    "tokeniser.train(text, 512, verbose=True)\n",
    "tokeniserregex.train(text, 512, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokeniser.encode(text)\n",
    "decoded = tokeniser.decode(ids)\n",
    "assert text==decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokeniserregex.encode(text)\n",
    "decoded = tokeniserregex.decode(ids)\n",
    "assert text==decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
